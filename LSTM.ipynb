{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: joblib in d:\\environment deep learning\\environments\\deep_learning\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\environment deep learning\\environments\\deep_learning\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in d:\\environment deep learning\\environments\\deep_learning\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in d:\\environment deep learning\\environments\\deep_learning\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # read the csv\n",
    "import re # regex to detect username, url, html entity \n",
    "import nltk # to use word tokenize (split the sentence into words)\n",
    "from nltk.corpus import stopwords # to remove the stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_E6oV3lV.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of tweets: (31962, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"num of tweets: {df.shape}\")\n",
    "\n",
    "tweet = list(df['tweet'])\n",
    "labels = list(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## notes: all of the function taking 1 text at a time\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# add rt to remove retweet in dataset (noise)\n",
    "stop_words.add(\"rt\")\n",
    "\n",
    "# remove html entity:\n",
    "def remove_entity(raw_text):\n",
    "    entity_regex = r\"&[^\\s;]+;\"\n",
    "    text = re.sub(entity_regex, \"\", raw_text)\n",
    "    return text\n",
    "\n",
    "# change the user tags\n",
    "def change_user(raw_text):\n",
    "    regex = r\"@([^ ]+)\"\n",
    "    text = re.sub(regex, \"user\", raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove urls\n",
    "def remove_url(raw_text):\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(url_regex, '', raw_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove unnecessary symbols\n",
    "def remove_noise_symbols(raw_text):\n",
    "    text = raw_text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"!\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"..\", '')\n",
    "\n",
    "    return text\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(raw_text):\n",
    "    tokenize = nltk.word_tokenize(raw_text)\n",
    "    text = [word for word in tokenize if not word.lower() in stop_words]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "## this function in to clean all the dataset by utilizing all the function above\n",
    "def preprocess(datas):\n",
    "    clean = []\n",
    "    # change the @xxx into \"user\"\n",
    "    clean = [change_user(text) for text in datas]\n",
    "    # remove emojis (specifically unicode emojis)\n",
    "    clean = [remove_entity(text) for text in clean]\n",
    "    # remove urls\n",
    "    clean = [remove_url(text) for text in clean]\n",
    "    # remove trailing stuff\n",
    "    clean = [remove_noise_symbols(text) for text in clean]\n",
    "    # remove stopwords\n",
    "    clean = [remove_stopwords(text) for text in clean]\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet = preprocess(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweet, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an outlier detected: [4116, 336, 54, 280, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "y_train = to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 200\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, output_dim, input_length=max_length),\n",
    "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 25, 200)           7895800   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                67840     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,972,347\n",
      "Trainable params: 7,972,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 39s 92ms/step - loss: 0.2042 - accuracy: 0.9454 - f1: 0.9267 - precision: 0.9331 - recall: 0.9243 - val_loss: 0.1210 - val_accuracy: 0.9576 - val_f1: 0.9577 - val_precision: 0.9578 - val_recall: 0.9576\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 41s 102ms/step - loss: 0.0609 - accuracy: 0.9801 - f1: 0.9800 - precision: 0.9802 - recall: 0.9799 - val_loss: 0.1396 - val_accuracy: 0.9629 - val_f1: 0.9629 - val_precision: 0.9631 - val_recall: 0.9628\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 40s 99ms/step - loss: 0.0204 - accuracy: 0.9931 - f1: 0.9931 - precision: 0.9932 - recall: 0.9931 - val_loss: 0.1795 - val_accuracy: 0.9562 - val_f1: 0.9564 - val_precision: 0.9567 - val_recall: 0.9562\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 41s 103ms/step - loss: 0.0089 - accuracy: 0.9970 - f1: 0.9970 - precision: 0.9970 - recall: 0.9969 - val_loss: 0.2284 - val_accuracy: 0.9607 - val_f1: 0.9607 - val_precision: 0.9607 - val_recall: 0.9607\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 41s 103ms/step - loss: 0.0048 - accuracy: 0.9986 - f1: 0.9986 - precision: 0.9986 - recall: 0.9986 - val_loss: 0.2577 - val_accuracy: 0.9598 - val_f1: 0.9598 - val_precision: 0.9598 - val_recall: 0.9598\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 50s 126ms/step - loss: 0.0035 - accuracy: 0.9989 - f1: 0.9989 - precision: 0.9990 - recall: 0.9989 - val_loss: 0.2944 - val_accuracy: 0.9573 - val_f1: 0.9573 - val_precision: 0.9573 - val_recall: 0.9573\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 44s 110ms/step - loss: 0.0023 - accuracy: 0.9995 - f1: 0.9995 - precision: 0.9995 - recall: 0.9995 - val_loss: 0.3298 - val_accuracy: 0.9589 - val_f1: 0.9589 - val_precision: 0.9589 - val_recall: 0.9589\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 55s 137ms/step - loss: 0.0021 - accuracy: 0.9994 - f1: 0.9994 - precision: 0.9994 - recall: 0.9994 - val_loss: 0.3252 - val_accuracy: 0.9598 - val_f1: 0.9597 - val_precision: 0.9598 - val_recall: 0.9596\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 53s 132ms/step - loss: 0.0020 - accuracy: 0.9994 - f1: 0.9994 - precision: 0.9994 - recall: 0.9994 - val_loss: 0.3674 - val_accuracy: 0.9596 - val_f1: 0.9592 - val_precision: 0.9599 - val_recall: 0.9586\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 41s 104ms/step - loss: 0.0027 - accuracy: 0.9992 - f1: 0.9992 - precision: 0.9992 - recall: 0.9992 - val_loss: 0.3496 - val_accuracy: 0.9587 - val_f1: 0.9587 - val_precision: 0.9590 - val_recall: 0.9584\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
